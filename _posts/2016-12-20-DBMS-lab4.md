---
layout: post
title: DBMG---Rapid Miner(Analysis of 3 classfication algorithms)
categories: [blog ]
tags: [study,DBMG,classfication, ]
description: 
---  

[good reference about decision tree](http://www.saedsayad.com/decision_tree.htm "good reference about decision tree")

Confidence and Support
Confidence and support are properties of rules. These statistical measures can be used to rank the rules and  
hence the predictions.

Support: The number of records in the training data set that satisfy the rule.

Confidence: The likelihood of the predicted outcome, given that the rule has been satisfied.

For example, consider a list of 1000 customers (1000 cases). Out of all the customers, 100 satisfy a given rule.  
Of these 100, 75 are likely to increase spending, and 25 are not likely to increase spending. The support of the  
rule is 100/1000 (10%). The confidence of the prediction (likely to increase spending) for the cases that satisfy  
the rule is 75/100 (75%).

**Pruning**  
Pruning is a way of reducing the size of the decision tree. This will reduce the accuracy on the training data,  
but (in general) increase the accuracy on unseen data. It is used to mitigate overfitting, where you would achieve   
perfect accuracy on training data

**Confidence in decision tree**  
This parameter specifies the confidence level used for the pessimistic error calculation of pruning. Range: real   
[Decision Tree Pruning based on Confidence Intervals](http://www.saedsayad.com/decision_tree.htm "good reference about decision tree")
The heart of the statistical pruning technique is the calculation of a confidence interval for the error rate. 
[Decision Tree Pruning based on Confidence Intervals](http://www.saedsayad.com/decision_tree_overfitting.htm)  
2nd reference used a graph clearly showed if the parent has an error estimation smaller than the weighted sum of error  
estimates for all its leaves, can prune that parent(In the formula, there is one parameter which is set in the range of  
confidence level)
