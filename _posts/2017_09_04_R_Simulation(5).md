### Monte Carlo Methods for Optimization Problems  
**Gradient ascent/descent**  
Gradient descent is a first-order iterative optimization algorithm for finding the minimum of a function. To find a local minimum of a function using gradient descent, one takes steps proportional to the negative of the gradient (or of the approximate gradient) of the function at the current point. If instead one takes steps proportional to the positive of the gradient, one approaches a local maximum of that function; the procedure is then known as gradient ascent.

Gradient descent is also known as steepest descent. However, gradient descent should not be confused with the method of steepest descent for approximating integrals.  

[Steepest Ascent](https://i.imgur.com/sM5a0hy.png)  
Steepest descent would just be in the opposite direction to the steepest ascent.  
[Steepest Example](https://i.imgur.com/Hpou35y.png)  
Note: D means deriavative   
[Steepest Example2](https://i.imgur.com/e1ZzKoy.png)
The length of the gradient measures the maximum possible rate of change rate. So in  
the opposite direction you just get the opposite value negative root 6, we can't go  
down any faster than that and so we won't be able to do it.  

